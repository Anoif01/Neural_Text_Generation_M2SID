{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16798,
     "status": "ok",
     "timestamp": 1607436425888,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "CxVs7VTsn4SL",
    "outputId": "60fbb93b-9124-46ac-ab4a-8954d60aaca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16797,
     "status": "ok",
     "timestamp": 1607436425890,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "7cIVgYew7uQb"
   },
   "outputs": [],
   "source": [
    "# ! cp -r /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data/data/. /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16796,
     "status": "ok",
     "timestamp": 1607436425891,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "Wtb9k0JFoDMc"
   },
   "outputs": [],
   "source": [
    "sys.path.append('/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25146,
     "status": "ok",
     "timestamp": 1607436434247,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "3zE4RyUVMkD0",
    "outputId": "537f5c83-f33d-4b73-c2bb-27468832c26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from copyattention import CopyAttention\n",
    "from input_data import DataSet\n",
    "from input_data import setup\n",
    "\n",
    "from feed_dicts import placeholder_inputs\n",
    "from feed_dicts import fill_feed_dict\n",
    "from feed_dicts import placeholder_inputs_single\n",
    "from feed_dicts import fill_feed_dict_single\n",
    "from feed_dicts import do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 25145,
     "status": "ok",
     "timestamp": 1607436434248,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "lHYogtgY4eJ3"
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/datas/rlebret-wikipedia-biography-dataset-d0d6c78')\n",
    "# os.getcwd()\n",
    "# !cat wikipedia-biography-dataset.z?? > tmp.zip\n",
    "# !unzip tmp.zip\n",
    "# !rm tmp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 25145,
     "status": "ok",
     "timestamp": 1607436434249,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "toFAVj8J7C6t"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 25143,
     "status": "ok",
     "timestamp": 1607436434249,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "_FYPTuZsMIsO"
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "# Model parameters\n",
    "flags.DEFINE_integer(\"n\", 14, \"n-gram model parameter [11]\") \n",
    "flags.DEFINE_integer(\"d\", 64, \"Dimension of word embeddings [64]\")\n",
    "flags.DEFINE_integer(\"g\", 128, \"Dimension of global embedding [128]\")\n",
    "flags.DEFINE_integer(\"nhu\", 256, \"Number of hidden units [256]\")\n",
    "flags.DEFINE_integer(\"l\", 10, \"Max number of words per field [10]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0025, \"Learning rate parameter [0.0025]\")\n",
    "\n",
    "# Dataset related parameters\n",
    "flags.DEFINE_integer(\"max_fields\", 10, \"Maximum number of fields in an infobox [10]\")\n",
    "flags.DEFINE_integer(\"word_max_fields\", 10, \"Maximum of fields a word from an infobox can appear in [10]\")\n",
    "flags.DEFINE_integer(\"nW\", 20000, \"Size of the sentence vocabulary\") # 20000\n",
    "flags.DEFINE_integer(\"min_field_freq\", 100, \"Minimum frequency of occurance of a field [100]\")\n",
    "\n",
    "# Temporary flags - To be fixed\n",
    "flags.DEFINE_integer(\"nQ\", 20000, \"Size of the table vocabulary\") # 20000\n",
    "flags.DEFINE_integer(\"nQpr\", 1000, \"Dummy\") # 1000\n",
    "\n",
    "# Experiment parameters\n",
    "flags.DEFINE_integer(\"num_epochs\", 10, \"Number of epochs [10]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for SGD [32]\") \n",
    "flags.DEFINE_integer(\"print_every\", 100, \"Print out the training loss every #steps [100]\")\n",
    "flags.DEFINE_integer(\"sample_every\", 1000, \"Sample sentences every #steps [1000]\")\n",
    "flags.DEFINE_integer(\"test_every\", 1000, \"Test after every #steps [1000]\")\n",
    "flags.DEFINE_integer(\"valid_every\", 1000, \"Validate after every #steps [1000]\")\n",
    "flags.DEFINE_string(\"data_dir\", '/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data', \"Path to the data directory [../data]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", '/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint', \"Directory to save checkpoints\")\n",
    "flags.DEFINE_string(\"experiment_dir\", '/content/drive/MyDrive/GM5/DEEP/Paper/experiment', \"Directory to store current experiment results\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "executionInfo": {
     "elapsed": 25137,
     "status": "ok",
     "timestamp": 1607436434250,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "8Du1xQanwD5l",
    "outputId": "40e06d35-9c7c-4254-df56-a3987c5e6f87",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'{\"logtostderr\": {\"name\": \"logtostderr\", \"help\": \"Should only log to stderr?\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"alsologtostderr\": {\"name\": \"alsologtostderr\", \"help\": \"also log to stderr?\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"log_dir\": {\"name\": \"log_dir\", \"help\": \"directory to write logfiles into\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"\", \"validators\": [], \"default_unparsed\": \"\", \"default\": \"\", \"default_as_str\": \"\\'\\'\"}, \"v\": {\"name\": \"verbosity\", \"help\": \"Logging verbosity level. Messages logged at this level or lower will be included. Set to 1 for debug logging. If the flag was not set or supplied, the value will be changed from the default of -1 (warning) to 0 (info) after flags are parsed.\", \"short_name\": \"v\", \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": true, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": -1, \"validators\": [], \"default_unparsed\": -1, \"default\": -1, \"default_as_str\": \"\\'-1\\'\"}, \"verbosity\": {\"name\": \"verbosity\", \"help\": \"Logging verbosity level. Messages logged at this level or lower will be included. Set to 1 for debug logging. If the flag was not set or supplied, the value will be changed from the default of -1 (warning) to 0 (info) after flags are parsed.\", \"short_name\": \"v\", \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": true, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": -1, \"validators\": [], \"default_unparsed\": -1, \"default\": -1, \"default_as_str\": \"\\'-1\\'\"}, \"logger_levels\": {\"name\": \"logger_levels\", \"help\": \"Specify log level of loggers. The format is a CSV list of `name:level`. Where `name` is the logger name used with `logging.getLogger()`, and `level` is a level name  (INFO, DEBUG, etc). e.g. `myapp.foo:INFO,other.logger:DEBUG`\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": {}, \"validators\": [], \"default_unparsed\": {}, \"default\": {}, \"default_as_str\": \"\\'\\'\"}, \"stderrthreshold\": {\"name\": \"stderrthreshold\", \"help\": \"log messages at this level, or more severe, to stderr in addition to the logfile.  Possible values are \\'debug\\', \\'info\\', \\'warning\\', \\'error\\', and \\'fatal\\'.  Obsoletes --alsologtostderr. Using --alsologtostderr cancels the effect of this flag. Please also note that this flag is subject to --verbosity and requires logfile not be stderr.\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": true, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"fatal\", \"validators\": [], \"default_unparsed\": \"fatal\", \"default\": \"fatal\", \"default_as_str\": \"\\'fatal\\'\"}, \"showprefixforinfo\": {\"name\": \"showprefixforinfo\", \"help\": \"If False, do not prepend prefix to info messages when it\\'s logged to stderr, --verbosity is set to INFO level, and python logging is used.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": true, \"validators\": [], \"default_unparsed\": true, \"default\": true, \"default_as_str\": \"\\'true\\'\"}, \"run_with_pdb\": {\"name\": \"run_with_pdb\", \"help\": \"Set to true for PDB debug mode\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"pdb_post_mortem\": {\"name\": \"pdb_post_mortem\", \"help\": \"Set to true to handle uncaught exceptions with PDB post mortem.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"pdb\": {\"name\": \"pdb\", \"help\": \"Alias for --pdb_post_mortem.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": null, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"run_with_profiling\": {\"name\": \"run_with_profiling\", \"help\": \"Set to true for profiling the script. Execution will be slower, and the output format might change over time.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"profile_file\": {\"name\": \"profile_file\", \"help\": \"Dump profile information to a file (for python -m pstats). Implies --run_with_profiling.\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": null, \"validators\": [], \"default_unparsed\": null, \"default\": null, \"default_as_str\": null}, \"use_cprofile_for_profiling\": {\"name\": \"use_cprofile_for_profiling\", \"help\": \"Use cProfile instead of the profile module for profiling. This has no effect unless --run_with_profiling is set.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": true, \"validators\": [], \"default_unparsed\": true, \"default\": true, \"default_as_str\": \"\\'true\\'\"}, \"only_check_args\": {\"name\": \"only_check_args\", \"help\": \"Set to true to validate args and exit.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": true, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"op_conversion_fallback_to_while_loop\": {\"name\": \"op_conversion_fallback_to_while_loop\", \"help\": \"If true, falls back to using a while loop for ops for which a converter is not defined.\", \"short_name\": null, \"boolean\": 1, \"present\": 0, \"parser\": {}, \"serializer\": null, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": false, \"validators\": [], \"default_unparsed\": false, \"default\": false, \"default_as_str\": \"\\'false\\'\"}, \"test_random_seed\": {\"name\": \"test_random_seed\", \"help\": \"Random seed for testing. Some test frameworks may change the default value of this flag between runs, so it is not appropriate for seeding probabilistic tests.\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 301, \"validators\": [], \"default_unparsed\": 301, \"default\": 301, \"default_as_str\": \"\\'301\\'\"}, \"test_srcdir\": {\"name\": \"test_srcdir\", \"help\": \"Root of directory tree where source files live\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"\", \"validators\": [], \"default_unparsed\": \"\", \"default\": \"\", \"default_as_str\": \"\\'\\'\"}, \"test_tmpdir\": {\"name\": \"test_tmpdir\", \"help\": \"Directory for temporary testing files\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"/tmp/absl_testing\", \"validators\": [], \"default_unparsed\": \"/tmp/absl_testing\", \"default\": \"/tmp/absl_testing\", \"default_as_str\": \"\\'/tmp/absl_testing\\'\"}, \"test_randomize_ordering_seed\": {\"name\": \"test_randomize_ordering_seed\", \"help\": \"If positive, use this as a seed to randomize the execution order for test cases. If \\\\\"random\\\\\", pick a random seed to use. If 0 or not set, do not randomize test case execution order. This flag also overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": true, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"\", \"validators\": [], \"default_unparsed\": \"\", \"default\": \"\", \"default_as_str\": \"\\'\\'\"}, \"xml_output_file\": {\"name\": \"xml_output_file\", \"help\": \"File to store XML test results\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"\", \"validators\": [], \"default_unparsed\": \"\", \"default\": \"\", \"default_as_str\": \"\\'\\'\"}, \"n\": {\"name\": \"n\", \"help\": \"n-gram model parameter [11]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 14, \"validators\": [], \"default_unparsed\": 14, \"default\": 14, \"default_as_str\": \"\\'14\\'\"}, \"d\": {\"name\": \"d\", \"help\": \"Dimension of word embeddings [64]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 64, \"validators\": [], \"default_unparsed\": 64, \"default\": 64, \"default_as_str\": \"\\'64\\'\"}, \"g\": {\"name\": \"g\", \"help\": \"Dimension of global embedding [128]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 128, \"validators\": [], \"default_unparsed\": 128, \"default\": 128, \"default_as_str\": \"\\'128\\'\"}, \"nhu\": {\"name\": \"nhu\", \"help\": \"Number of hidden units [256]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 256, \"validators\": [], \"default_unparsed\": 256, \"default\": 256, \"default_as_str\": \"\\'256\\'\"}, \"l\": {\"name\": \"l\", \"help\": \"Max number of words per field [10]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 10, \"validators\": [], \"default_unparsed\": 10, \"default\": 10, \"default_as_str\": \"\\'10\\'\"}, \"learning_rate\": {\"name\": \"learning_rate\", \"help\": \"Learning rate parameter [0.0025]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"a number\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 0.0025, \"validators\": [], \"default_unparsed\": 0.0025, \"default\": 0.0025, \"default_as_str\": \"\\'0.0025\\'\"}, \"max_fields\": {\"name\": \"max_fields\", \"help\": \"Maximum number of fields in an infobox [10]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 10, \"validators\": [], \"default_unparsed\": 10, \"default\": 10, \"default_as_str\": \"\\'10\\'\"}, \"word_max_fields\": {\"name\": \"word_max_fields\", \"help\": \"Maximum of fields a word from an infobox can appear in [10]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 10, \"validators\": [], \"default_unparsed\": 10, \"default\": 10, \"default_as_str\": \"\\'10\\'\"}, \"nW\": {\"name\": \"nW\", \"help\": \"Size of the sentence vocabulary\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 20000, \"validators\": [], \"default_unparsed\": 20000, \"default\": 20000, \"default_as_str\": \"\\'20000\\'\"}, \"min_field_freq\": {\"name\": \"min_field_freq\", \"help\": \"Minimum frequency of occurance of a field [100]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 100, \"validators\": [], \"default_unparsed\": 100, \"default\": 100, \"default_as_str\": \"\\'100\\'\"}, \"nQ\": {\"name\": \"nQ\", \"help\": \"Size of the table vocabulary\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 20000, \"validators\": [], \"default_unparsed\": 20000, \"default\": 20000, \"default_as_str\": \"\\'20000\\'\"}, \"nQpr\": {\"name\": \"nQpr\", \"help\": \"Dummy\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 1000, \"validators\": [], \"default_unparsed\": 1000, \"default\": 1000, \"default_as_str\": \"\\'1000\\'\"}, \"num_epochs\": {\"name\": \"num_epochs\", \"help\": \"Number of epochs [10]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 10, \"validators\": [], \"default_unparsed\": 10, \"default\": 10, \"default_as_str\": \"\\'10\\'\"}, \"batch_size\": {\"name\": \"batch_size\", \"help\": \"Batch size for SGD [32]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 32, \"validators\": [], \"default_unparsed\": 32, \"default\": 32, \"default_as_str\": \"\\'32\\'\"}, \"print_every\": {\"name\": \"print_every\", \"help\": \"Print out the training loss every #steps [100]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 100, \"validators\": [], \"default_unparsed\": 100, \"default\": 100, \"default_as_str\": \"\\'100\\'\"}, \"sample_every\": {\"name\": \"sample_every\", \"help\": \"Sample sentences every #steps [1000]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 1000, \"validators\": [], \"default_unparsed\": 1000, \"default\": 1000, \"default_as_str\": \"\\'1000\\'\"}, \"test_every\": {\"name\": \"test_every\", \"help\": \"Test after every #steps [1000]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 1000, \"validators\": [], \"default_unparsed\": 1000, \"default\": 1000, \"default_as_str\": \"\\'1000\\'\"}, \"valid_every\": {\"name\": \"valid_every\", \"help\": \"Validate after every #steps [1000]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {\"lower_bound\": null, \"upper_bound\": null, \"syntactic_help\": \"an integer\"}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": 1000, \"validators\": [], \"default_unparsed\": 1000, \"default\": 1000, \"default_as_str\": \"\\'1000\\'\"}, \"data_dir\": {\"name\": \"data_dir\", \"help\": \"Path to the data directory [../data]\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data\", \"validators\": [], \"default_unparsed\": \"/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data\", \"default\": \"/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data\", \"default_as_str\": \"\\'/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data\\'\"}, \"checkpoint_dir\": {\"name\": \"checkpoint_dir\", \"help\": \"Directory to save checkpoints\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint\", \"validators\": [], \"default_unparsed\": \"/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint\", \"default\": \"/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint\", \"default_as_str\": \"\\'/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint\\'\"}, \"experiment_dir\": {\"name\": \"experiment_dir\", \"help\": \"Directory to store current experiment results\", \"short_name\": null, \"boolean\": false, \"present\": 0, \"parser\": {}, \"serializer\": {}, \"allow_override\": false, \"allow_override_cpp\": false, \"allow_hide_cpp\": false, \"allow_overwrite\": true, \"allow_using_method_names\": false, \"using_default_value\": true, \"_value\": \"/content/drive/MyDrive/GM5/DEEP/Paper/experiment\", \"validators\": [], \"default_unparsed\": \"/content/drive/MyDrive/GM5/DEEP/Paper/experiment\", \"default\": \"/content/drive/MyDrive/GM5/DEEP/Paper/experiment\", \"default_as_str\": \"\\'/content/drive/MyDrive/GM5/DEEP/Paper/experiment\\'\"}}'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gérer le pb de JsonEcoder\n",
    "from json import JSONEncoder\n",
    "class MyEncoder(JSONEncoder):\n",
    "  def default(self, o):\n",
    "    return o.__dict__\n",
    "\n",
    "MyEncoder().encode(flags.FLAGS.__flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction implémentée pour calculer BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 26782,
     "status": "ok",
     "timestamp": 1607436435897,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "X4Nf0_7l2_HZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def split(sentence): \n",
    "    return (sentence.split())\n",
    "\n",
    "def compute_BLEU(sentence, ref):\n",
    "  reference = split(ref)\n",
    "  # if sentence == '. \\n':\n",
    "  #     return 0\n",
    "  # else:\n",
    "  if '-lrb-' in sentence:\n",
    "      reference.append('-lrb-')\n",
    "  if '-rrb-' in sentence:\n",
    "      reference.append('-rrb-')\n",
    "\n",
    "  candidate = split(sentence)\n",
    "  # print(reference)\n",
    "  # print(candidate)\n",
    "  BLEU_score = sentence_bleu([reference], candidate)\n",
    "  return BLEU_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première version de main\n",
    "<br>\n",
    "Nous calculons pas de BLEU, ni la perpléxité. Nous enregistrons les pertes d'entraînement et de validation dans les fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26782,
     "status": "ok",
     "timestamp": 1607436435898,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "b4JKu4l1Uic7"
   },
   "outputs": [],
   "source": [
    "def main_v1(_):\n",
    "    # pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    #### experiment_dir set : choose the last experiment file.\n",
    "    if not os.path.exists(FLAGS.experiment_dir):\n",
    "        os.makedirs(FLAGS.experiment_dir)\n",
    "        expt_num = \"1\"\n",
    "    else:\n",
    "        expts = os.listdir(FLAGS.experiment_dir)\n",
    "        last_expr = max([int(folder) for folder in expts])\n",
    "        expt_num = str(last_expr + 1)\n",
    "    expt_result_path = os.path.join(FLAGS.experiment_dir, expt_num)\n",
    "    os.makedirs(expt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### checkpoint_dir path set\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    chkpt_result_path = os.path.join(FLAGS.checkpoint_dir, expt_num)\n",
    "    os.makedirs(chkpt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### record flags.FLAGS --> write them into params.json\n",
    "    params_e_path = os.path.join(expt_result_path, \"params.json\")\n",
    "    params_c_path = os.path.join(chkpt_result_path, \"params.json\")\n",
    "    with open(params_e_path, 'w') as params_e, open(params_c_path, 'w') as params_c:\n",
    "        json.dump(flags.FLAGS.__flags, params_e,cls= MyEncoder)\n",
    "        json.dump(flags.FLAGS.__flags, params_c,cls= MyEncoder)\n",
    "    ####\n",
    "\n",
    "    #### Generate the indexes for create train/valid/test dataset objects\n",
    "    word2idx, field2idx, qword2idx, nF, max_words_in_table, word_set = \\\n",
    "        setup(FLAGS.data_dir, '/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data/embeddings', FLAGS.n, FLAGS.batch_size,FLAGS.nW, FLAGS.min_field_freq, FLAGS.nQ)\n",
    "\n",
    "    train_dataset = DataSet(FLAGS.data_dir, 'train', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    \n",
    "    num_train_examples = int(train_dataset.num_examples()/100)  ##### on prend seulement 1% de dataset\n",
    "\n",
    "    valid_dataset = DataSet(FLAGS.data_dir, 'valid', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    test_dataset = DataSet(FLAGS.data_dir, 'test', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "    # print(test_dataset._xs[:10])\n",
    "    # print(test_dataset._ys[:10])\n",
    "    \n",
    "    print('num_train_examples: ', num_train_examples)\n",
    "    print('num_valid_examples: ', int(.1*valid_dataset.num_examples()))\n",
    "    print('num_test_examples: ',int(.1*test_dataset.num_examples()))\n",
    "    ####\n",
    "\n",
    "\n",
    "    #### The sizes of respective conditioning variables for placeholder generation\n",
    "    context_size = (FLAGS.n - 1)\n",
    "    zp_size = context_size * FLAGS.word_max_fields\n",
    "    zm_size = context_size * FLAGS.word_max_fields\n",
    "    gf_size = FLAGS.max_fields\n",
    "    gw_size = max_words_in_table\n",
    "    copy_size = FLAGS.word_max_fields\n",
    "    proj_size = FLAGS.nW + max_words_in_table\n",
    "    ####\n",
    "\n",
    "    #### Generate the TensorFlow graph\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        ## Create the CopyAttention model\n",
    "        start_c = time.time()\n",
    "        model = CopyAttention(FLAGS.n, FLAGS.d, FLAGS.g, FLAGS.nhu,\n",
    "                              FLAGS.nW, nF, FLAGS.nQ, FLAGS.l,\n",
    "                              FLAGS.learning_rate, max_words_in_table,\n",
    "                              FLAGS.max_fields, FLAGS.word_max_fields)\n",
    "        duration_c = time.time() - start_c\n",
    "        print(\"======= CopyAttention model done in %.3f minutes. =======\"%(duration_c/60))\n",
    "        ##\n",
    "\n",
    "\n",
    "        ## Placeholders for train and validation with known shape per batch\n",
    "        # context_pl (32, context_size) ; zp_pl (32, zp_size); zm_pl (32, zm_size)\n",
    "        # gf_pl (32, gf_size) ; gw_pl (32, gw_size); copy_pl (none, copy_size)\n",
    "        # proj_pl (32, proj_size); next_pl True next word tensor\n",
    "        context_pl, zp_pl, zm_pl, gf_pl, gw_pl, next_pl, copy_pl, proj_pl = \\\n",
    "            placeholder_inputs(FLAGS.batch_size, context_size, zp_size,\n",
    "                                zm_size, gf_size, gw_size, copy_size,\n",
    "                                proj_size)\n",
    "        # Placeholders for test\n",
    "        context_plt, zp_plt, zm_plt, gf_plt, gw_plt, copy_plt, proj_plt, next_plt = \\\n",
    "            placeholder_inputs_single(context_size, zp_size, zm_size,\n",
    "                                      gf_size, gw_size, copy_size,\n",
    "                                      proj_size)\n",
    "            \n",
    "        print(\"======= Verify placeholders: context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl: =======\")\n",
    "        for i in [context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl]:\n",
    "            print(i.get_shape())\n",
    "        ##\n",
    "\n",
    "        \n",
    "        ## Train and validation part of the CopyAttention model\n",
    "        # print(\"======= Training with batch size = %d =======\"%FLAGS.batch_size)\n",
    "        predict = model.inference(FLAGS.batch_size, context_pl, zp_pl, zm_pl,\n",
    "                                  gf_pl, gw_pl, copy_pl, proj_pl)\n",
    "        loss = model.loss(predict, next_pl) # cross_entropy\n",
    "        train_op = model.training(loss) # optimizer\n",
    "        evaluate = model.evaluate(predict, next_pl)\n",
    "        # print(\"Train Accuracy: \", evaluate)\n",
    "        # print(\"======= Stop Training =======\")\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Test component of the model\n",
    "        # print(\"======= Testing model with batch size = 1 =======\")\n",
    "        pred_single = model.inference(1, context_plt, zp_plt, zm_plt,\n",
    "                                      gf_plt, gw_plt, copy_plt,\n",
    "                                      proj_plt)\n",
    "        predicted_label = model.predict(pred_single) #label, softmax pred_single\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Initialize the variables and start the session\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        # ckpt_file = os.path.join('/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint','15', '16.ckpt')\n",
    "        # saver.restore(sess, r'/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint/15/16.ckpt')\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "        # train_loss_epoch = []\n",
    "        \n",
    "        for epoch in range(1, FLAGS.num_epochs + 1):\n",
    "            train_loss_tot = []\n",
    "\n",
    "            train_dataset.generate_permutation()\n",
    "            start_e = time.time()\n",
    "            for i in range(num_train_examples):\n",
    "                try:\n",
    "                    feed_dict = fill_feed_dict(train_dataset, i,\n",
    "                                              context_pl, zp_pl, zm_pl,\n",
    "                                              gf_pl, gw_pl, next_pl,\n",
    "                                              copy_pl, proj_pl)\n",
    "                    _, loss_value = sess.run([train_op, loss],\n",
    "                                            feed_dict=feed_dict)\n",
    "\n",
    "                    train_loss_tot.append(loss_value)\n",
    "\n",
    "                    if i % FLAGS.print_every == 0:\n",
    "                        print(\"Epoch : %d\\tStep : %d\\tLoss : %0.3f\" % (epoch, i, loss_value))\n",
    "\n",
    "                    if i == -1 and i % FLAGS.valid_every == 0:\n",
    "                        print(\"Validation starting\")\n",
    "                        #### TEST\n",
    "                        valid_loss = do_eval(sess, train_op, loss,\n",
    "                                            valid_dataset,\n",
    "                                            context_pl, zp_pl, zm_pl,\n",
    "                                            gf_pl, gw_pl, next_pl,\n",
    "                                            copy_pl, proj_pl)\n",
    "                        print(\"Epoch : %d\\tValidation loss: %0.5f\" % (i, valid_loss))\n",
    "\n",
    "                    if i != 0 :#and i % FLAGS.sample_every == 0:\n",
    "                        # print(\"Test starting\")\n",
    "                        test_dataset.reset_context()\n",
    "                        pos = 0\n",
    "                        len_sent = 0\n",
    "                        prev_predict = word2idx['<start>']\n",
    "                        res_path = os.path.join(expt_result_path, 'sample%d.txt'%epoch)\n",
    "                        print(\"Write every 1000 sample in txt\")\n",
    "                        with open(res_path, 'a') as exp:\n",
    "                            while pos != 1:\n",
    "                                try:\n",
    "                                  #### TEST\n",
    "                                    feed_dict_t, idx2wq = fill_feed_dict_single(test_dataset,\n",
    "                                                                                prev_predict,\n",
    "                                                                                0, context_plt,\n",
    "                                                                                zp_plt, zm_plt,\n",
    "                                                                                gf_plt, gw_plt,\n",
    "                                                                                next_plt,\n",
    "                                                                                copy_plt,\n",
    "                                                                                proj_plt)\n",
    "                                    prev_predict = sess.run([predicted_label],\n",
    "                                                            feed_dict=feed_dict_t)\n",
    "                                    prev = prev_predict[0][0][0]\n",
    "                                    if prev in idx2wq:\n",
    "                                        exp.write(idx2wq[prev] + ' ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    else:\n",
    "                                        exp.write('<unk> ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    if prev == word2idx['.']:\n",
    "                                        pos = 1\n",
    "                                        exp.write('\\n')\n",
    "                                    if len_sent == 50:\n",
    "                                        break\n",
    "                                    prev_predict = prev\n",
    "                                except:\n",
    "                                    pass\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # train_loss_epoch.append(train_loss_epoch.mean())\n",
    "            duration_e = time.time() - start_e\n",
    "            print(\"Time taken for epoch : %d is %.3f minutes\" % (epoch, duration_e/60))\n",
    "\n",
    "\n",
    "            train_res = os.path.join(expt_result_path, 'train_loss.txt')\n",
    "            with open(train_res, 'a') as tloss_f:\n",
    "                tloss_f.write(\"Epoch : %d\\tTrain loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, np.mean(train_loss_tot), duration_e))\n",
    "\n",
    "            print(\"Saving checkpoint for epoch %d\" % (epoch))\n",
    "            checkpoint_file = os.path.join(chkpt_result_path, str(epoch) + '.ckpt')\n",
    "            saver.save(sess, checkpoint_file)\n",
    "\n",
    "            print(\"Validation starting\")\n",
    "            start = time.time()\n",
    "            valid_loss = do_eval(sess, train_op, loss,\n",
    "                                 valid_dataset, context_pl,\n",
    "                                 zp_pl, zm_pl, gf_pl, gw_pl,\n",
    "                                 next_pl, copy_pl, proj_pl)\n",
    "            duration = time.time() - start\n",
    "            print(\"Epoch : %d\\tValidation loss: %0.5f\" % (epoch, valid_loss))\n",
    "            print(\"Time taken for validating epoch %d : %0.3f\" % (epoch, duration))\n",
    "            valid_res = os.path.join(expt_result_path, 'valid_loss.txt')\n",
    "            \n",
    "            with open(valid_res, 'a') as vloss_f:\n",
    "                vloss_f.write(\"Epoch : %d\\tValidation loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, valid_loss, duration))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième version de main\n",
    "<br>\n",
    "Nous ajoutons le méchanisme \"Early stopping\" dans le main. Nous enregistrons les pertes d'entraînement et de validation dans les fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26781,
     "status": "ok",
     "timestamp": 1607436435899,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "Fn60_VxvuzX2"
   },
   "outputs": [],
   "source": [
    "def main_v2(_):\n",
    "    # pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    #### experiment_dir set : choose the last experiment file.\n",
    "    if not os.path.exists(FLAGS.experiment_dir):\n",
    "        os.makedirs(FLAGS.experiment_dir)\n",
    "        expt_num = \"1\"\n",
    "    else:\n",
    "        expts = os.listdir(FLAGS.experiment_dir)\n",
    "        last_expr = max([int(folder) for folder in expts])\n",
    "        expt_num = str(last_expr + 1)\n",
    "    expt_result_path = os.path.join(FLAGS.experiment_dir, expt_num)\n",
    "    os.makedirs(expt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### checkpoint_dir path set\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    chkpt_result_path = os.path.join(FLAGS.checkpoint_dir, expt_num)\n",
    "    os.makedirs(chkpt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### write them into params.json\n",
    "    params_e_path = os.path.join(expt_result_path, \"params.json\")\n",
    "    params_c_path = os.path.join(chkpt_result_path, \"params.json\")\n",
    "    with open(params_e_path, 'w') as params_e, open(params_c_path, 'w') as params_c:\n",
    "        json.dump(flags.FLAGS.__flags, params_e,cls= MyEncoder)\n",
    "        json.dump(flags.FLAGS.__flags, params_c,cls= MyEncoder)\n",
    "    ####\n",
    "\n",
    "    #### Generate the indexes for create train/valid/test dataset objects\n",
    "    word2idx, field2idx, qword2idx, nF, max_words_in_table, word_set = \\\n",
    "        setup(FLAGS.data_dir, '/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data/embeddings', FLAGS.n, FLAGS.batch_size,FLAGS.nW, FLAGS.min_field_freq, FLAGS.nQ)\n",
    "\n",
    "    train_dataset = DataSet(FLAGS.data_dir, 'train', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    \n",
    "    num_train_examples = int(train_dataset.num_examples()/100)  ##### on prend seulement 1% de dataset\n",
    "\n",
    "    valid_dataset = DataSet(FLAGS.data_dir, 'valid', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    test_dataset = DataSet(FLAGS.data_dir, 'test', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "    # print(test_dataset._xs[:10])\n",
    "    # print(test_dataset._ys[:10])\n",
    "    \n",
    "    print('num_train_examples: ', num_train_examples)\n",
    "    print('num_valid_examples: ', int(.1*valid_dataset.num_examples()))\n",
    "    print('num_test_examples: ',int(.1*test_dataset.num_examples()))\n",
    "    ####\n",
    "\n",
    "\n",
    "    #### The sizes of respective conditioning variables for placeholder generation\n",
    "    context_size = (FLAGS.n - 1)\n",
    "    zp_size = context_size * FLAGS.word_max_fields\n",
    "    zm_size = context_size * FLAGS.word_max_fields\n",
    "    gf_size = FLAGS.max_fields\n",
    "    gw_size = max_words_in_table\n",
    "    copy_size = FLAGS.word_max_fields\n",
    "    proj_size = FLAGS.nW + max_words_in_table\n",
    "    ####\n",
    "\n",
    "    #### Generate the TensorFlow graph\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        ## Create the CopyAttention model\n",
    "        start_c = time.time()\n",
    "        model = CopyAttention(FLAGS.n, FLAGS.d, FLAGS.g, FLAGS.nhu,\n",
    "                              FLAGS.nW, nF, FLAGS.nQ, FLAGS.l,\n",
    "                              FLAGS.learning_rate, max_words_in_table,\n",
    "                              FLAGS.max_fields, FLAGS.word_max_fields)\n",
    "        duration_c = time.time() - start_c\n",
    "        print(\"======= CopyAttention model done in %.3f minutes. =======\"%(duration_c/60))\n",
    "        ##\n",
    "\n",
    "\n",
    "        ## Placeholders for train and validation with known shape per batch\n",
    "        # context_pl (32, context_size) ; zp_pl (32, zp_size); zm_pl (32, zm_size)\n",
    "        # gf_pl (32, gf_size) ; gw_pl (32, gw_size); copy_pl (none, copy_size)\n",
    "        # proj_pl (32, proj_size); next_pl True next word tensor\n",
    "        context_pl, zp_pl, zm_pl, gf_pl, gw_pl, next_pl, copy_pl, proj_pl = \\\n",
    "            placeholder_inputs(FLAGS.batch_size, context_size, zp_size,\n",
    "                                zm_size, gf_size, gw_size, copy_size,\n",
    "                                proj_size)\n",
    "        # Placeholders for test\n",
    "        context_plt, zp_plt, zm_plt, gf_plt, gw_plt, copy_plt, proj_plt, next_plt = \\\n",
    "            placeholder_inputs_single(context_size, zp_size, zm_size,\n",
    "                                      gf_size, gw_size, copy_size,\n",
    "                                      proj_size)\n",
    "            \n",
    "        print(\"======= Verify placeholders: context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl: =======\")\n",
    "        for i in [context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl]:\n",
    "            print(i.get_shape())\n",
    "        ##\n",
    "\n",
    "        \n",
    "        ## Train and validation part of the CopyAttention model\n",
    "        # print(\"======= Training with batch size = %d =======\"%FLAGS.batch_size)\n",
    "        predict = model.inference(FLAGS.batch_size, context_pl, zp_pl, zm_pl,\n",
    "                                  gf_pl, gw_pl, copy_pl, proj_pl)\n",
    "        loss = model.loss(predict, next_pl) # cross_entropy\n",
    "        train_op = model.training(loss) # optimizer\n",
    "        evaluate = model.evaluate(predict, next_pl)\n",
    "        # print(\"Train Accuracy: \", evaluate)\n",
    "        # print(\"======= Stop Training =======\")\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Test component of the model\n",
    "        # print(\"======= Testing model with batch size = 1 =======\")\n",
    "        pred_single = model.inference(1, context_plt, zp_plt, zm_plt,\n",
    "                                      gf_plt, gw_plt, copy_plt,\n",
    "                                      proj_plt)\n",
    "        predicted_label = model.predict(pred_single) #预测label, softmax pred_single\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Initialize the variables and start the session\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        # ckpt_file = os.path.join('/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint','15', '16.ckpt')\n",
    "        # saver.restore(sess, r'/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint/15/16.ckpt')\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "        # train_loss_epoch = []\n",
    "        sigma = 1e-4\n",
    "        epoch = 1\n",
    "        best_loss = 100.\n",
    "        best_epoch = 0\n",
    "        diff_epoch = 0\n",
    "        while (epoch < FLAGS.num_epochs + 1) and (diff_epoch < 4):\n",
    "        # for epoch in range(1, FLAGS.num_epochs + 1):\n",
    "            train_loss_tot = []\n",
    "            train_dataset.generate_permutation()\n",
    "            start_e = time.time()\n",
    "            for i in range(num_train_examples):\n",
    "                try:\n",
    "                    feed_dict = fill_feed_dict(train_dataset, i,\n",
    "                                              context_pl, zp_pl, zm_pl,\n",
    "                                              gf_pl, gw_pl, next_pl,\n",
    "                                              copy_pl, proj_pl)\n",
    "                    _, loss_value = sess.run([train_op, loss],\n",
    "                                            feed_dict=feed_dict)\n",
    "                    \n",
    "                    train_loss_tot.append(loss_value)\n",
    "\n",
    "                    if i % FLAGS.print_every == 0:\n",
    "                        print(\"Epoch : %d\\tStep : %d\\tLoss : %0.3f\" % (epoch, i, loss_value))\n",
    "\n",
    "                    if i == -1 and i % FLAGS.valid_every == 0:\n",
    "                        print(\"Validation starting\")\n",
    "                        #### TEST\n",
    "                        valid_loss = do_eval(sess, train_op, loss,\n",
    "                                            valid_dataset,\n",
    "                                            context_pl, zp_pl, zm_pl,\n",
    "                                            gf_pl, gw_pl, next_pl,\n",
    "                                            copy_pl, proj_pl)\n",
    "                        print(\"Epoch : %d\\tValidation loss: %0.5f\" % (i, valid_loss))\n",
    "\n",
    "                    if i != 0:# and i % FLAGS.sample_every == 0:\n",
    "                        # print(\"Test starting\")\n",
    "                        test_dataset.reset_context()\n",
    "                        pos = 0\n",
    "                        len_sent = 0\n",
    "                        prev_predict = word2idx['<start>']\n",
    "                        res_path = os.path.join(expt_result_path, 'sample%d.txt'%epoch)\n",
    "                        print(\"Write every 1000 sample in txt\")\n",
    "                        with open(res_path, 'a') as exp:\n",
    "                            while pos != 1:\n",
    "                                try:\n",
    "                                  #### TEST\n",
    "                                    feed_dict_t, idx2wq = fill_feed_dict_single(test_dataset,\n",
    "                                                                                prev_predict,\n",
    "                                                                                0, context_plt,\n",
    "                                                                                zp_plt, zm_plt,\n",
    "                                                                                gf_plt, gw_plt,\n",
    "                                                                                next_plt,\n",
    "                                                                                copy_plt,\n",
    "                                                                                proj_plt)\n",
    "                                    prev_predict = sess.run([predicted_label],\n",
    "                                                            feed_dict=feed_dict_t)\n",
    "                                    prev = prev_predict[0][0][0]\n",
    "                                    if prev in idx2wq:\n",
    "                                        exp.write(idx2wq[prev] + ' ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    else:\n",
    "                                        exp.write('<unk> ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    if prev == word2idx['.']:\n",
    "                                        pos = 1\n",
    "                                        exp.write('\\n')\n",
    "                                    if len_sent == 50:\n",
    "                                        break\n",
    "                                    prev_predict = prev\n",
    "                                except:\n",
    "                                    pass\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # train_loss_epoch.append(train_loss_epoch.mean())\n",
    "            duration_e = time.time() - start_e\n",
    "            print(\"Time taken for epoch : %d is %.3f minutes\" % (epoch, duration_e/60))\n",
    "\n",
    "\n",
    "            train_res = os.path.join(expt_result_path, 'train_loss.txt')\n",
    "            with open(train_res, 'a') as tloss_f:\n",
    "                tloss_f.write(\"Epoch : %d\\tTrain loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, np.mean(train_loss_tot), duration_e))\n",
    "\n",
    "            print(\"Saving checkpoint for epoch %d\" % (epoch))\n",
    "            checkpoint_file = os.path.join(chkpt_result_path, str(epoch) + '.ckpt')\n",
    "            saver.save(sess, checkpoint_file)\n",
    "\n",
    "            print(\"Validation starting\")\n",
    "            start = time.time()\n",
    "            valid_loss = do_eval(sess, train_op, loss,\n",
    "                                 valid_dataset, context_pl,\n",
    "                                 zp_pl, zm_pl, gf_pl, gw_pl,\n",
    "                                 next_pl, copy_pl, proj_pl)\n",
    "            duration = time.time() - start\n",
    "            print(\"Epoch : %d\\tValidation loss: %0.5f\" % (epoch, valid_loss))\n",
    "            print(\"Time taken for validating epoch %d : %0.3f\" % (epoch, duration))\n",
    "            valid_res = os.path.join(expt_result_path, 'valid_loss.txt')\n",
    "            \n",
    "            with open(valid_res, 'a') as vloss_f:\n",
    "                vloss_f.write(\"Epoch : %d\\tValidation loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, valid_loss, duration))\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_epoch = epoch    \n",
    "            diff_epoch = epoch - best_epoch\n",
    "            epoch += 1\n",
    "            if diff_epoch>=4:\n",
    "                print(\"====Early Stopping====\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troisième version de main\n",
    "<br>\n",
    "Nous calculons le BLEU et la perpléxité. Nous enregistrons les pertes d'entraînement et de validation dans les fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 27609,
     "status": "ok",
     "timestamp": 1607436436729,
     "user": {
      "displayName": "Evol Lu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVDx-hjW9a0BZRpMhoIGipjiLhvi16x4b1PYQC=s64",
      "userId": "05833661288838512141"
     },
     "user_tz": -60
    },
    "id": "nOIgWUwzhXAM"
   },
   "outputs": [],
   "source": [
    "def main_v3(_):\n",
    "    # pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    #### experiment_dir set : choose the last experiment file.\n",
    "    if not os.path.exists(FLAGS.experiment_dir):\n",
    "        os.makedirs(FLAGS.experiment_dir)\n",
    "        expt_num = \"1\"\n",
    "    else:\n",
    "        expts = os.listdir(FLAGS.experiment_dir)\n",
    "        last_expr = max([int(folder) for folder in expts])\n",
    "        expt_num = str(last_expr + 1)\n",
    "    expt_result_path = os.path.join(FLAGS.experiment_dir, expt_num)\n",
    "    os.makedirs(expt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### checkpoint_dir path set\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    chkpt_result_path = os.path.join(FLAGS.checkpoint_dir, expt_num)\n",
    "    os.makedirs(chkpt_result_path)\n",
    "    ####\n",
    "\n",
    "    #### write them into params.json\n",
    "    params_e_path = os.path.join(expt_result_path, \"params.json\")\n",
    "    params_c_path = os.path.join(chkpt_result_path, \"params.json\")\n",
    "    with open(params_e_path, 'w') as params_e, open(params_c_path, 'w') as params_c:\n",
    "        json.dump(flags.FLAGS.__flags, params_e,cls= MyEncoder)\n",
    "        json.dump(flags.FLAGS.__flags, params_c,cls= MyEncoder)\n",
    "    ####\n",
    "\n",
    "    #### Generate the indexes for create train/valid/test dataset objects\n",
    "    word2idx, field2idx, qword2idx, nF, max_words_in_table, word_set = \\\n",
    "        setup(FLAGS.data_dir, '/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data/embeddings', FLAGS.n, FLAGS.batch_size,FLAGS.nW, FLAGS.min_field_freq, FLAGS.nQ)\n",
    "\n",
    "    train_dataset = DataSet(FLAGS.data_dir, 'train', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    \n",
    "    num_train_examples = int(train_dataset.num_examples()/20)  ##### on prend seulement 1% de dataset\n",
    "\n",
    "    valid_dataset = DataSet(FLAGS.data_dir, 'valid', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "\n",
    "    test_dataset = DataSet(FLAGS.data_dir, 'test', FLAGS.n, FLAGS.nW, nF,\n",
    "                            FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "                            field2idx, qword2idx,\n",
    "                            FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "                            max_words_in_table, word_set)\n",
    "    # print(test_dataset._xs[:10])\n",
    "    # print(test_dataset._ys[:10])\n",
    "    \n",
    "    print('num_train_examples: ', num_train_examples)\n",
    "    print('num_valid_examples: ', int(.1*valid_dataset.num_examples()))\n",
    "    print('num_test_examples: ',int(.1*test_dataset.num_examples()))\n",
    "    ####\n",
    "\n",
    "\n",
    "    #### The sizes of respective conditioning variables for placeholder generation\n",
    "    context_size = (FLAGS.n - 1)\n",
    "    zp_size = context_size * FLAGS.word_max_fields\n",
    "    zm_size = context_size * FLAGS.word_max_fields\n",
    "    gf_size = FLAGS.max_fields\n",
    "    gw_size = max_words_in_table\n",
    "    copy_size = FLAGS.word_max_fields\n",
    "    proj_size = FLAGS.nW + max_words_in_table\n",
    "    ####\n",
    "\n",
    "    #### Generate the TensorFlow graph\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        ## Create the CopyAttention model\n",
    "        start_c = time.time()\n",
    "        model = CopyAttention(FLAGS.n, FLAGS.d, FLAGS.g, FLAGS.nhu,\n",
    "                              FLAGS.nW, nF, FLAGS.nQ, FLAGS.l,\n",
    "                              FLAGS.learning_rate, max_words_in_table,\n",
    "                              FLAGS.max_fields, FLAGS.word_max_fields)\n",
    "        duration_c = time.time() - start_c\n",
    "        print(\"======= CopyAttention model done in %.3f minutes. =======\"%(duration_c/60))\n",
    "        ##\n",
    "\n",
    "\n",
    "        ## Placeholders for train and validation with known shape per batch\n",
    "        # context_pl (32, context_size) ; zp_pl (32, zp_size); zm_pl (32, zm_size)\n",
    "        # gf_pl (32, gf_size) ; gw_pl (32, gw_size); copy_pl (none, copy_size)\n",
    "        # proj_pl (32, proj_size); next_pl True next word tensor\n",
    "        context_pl, zp_pl, zm_pl, gf_pl, gw_pl, next_pl, copy_pl, proj_pl = \\\n",
    "            placeholder_inputs(FLAGS.batch_size, context_size, zp_size,\n",
    "                                zm_size, gf_size, gw_size, copy_size,\n",
    "                                proj_size)\n",
    "        # Placeholders for test\n",
    "        context_plt, zp_plt, zm_plt, gf_plt, gw_plt, copy_plt, proj_plt, next_plt = \\\n",
    "            placeholder_inputs_single(context_size, zp_size, zm_size,\n",
    "                                      gf_size, gw_size, copy_size,\n",
    "                                      proj_size)\n",
    "            \n",
    "        print(\"======= Verify placeholders: context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl: =======\")\n",
    "        for i in [context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl]:\n",
    "            print(i.get_shape())\n",
    "        ##\n",
    "\n",
    "        \n",
    "        ## Train and validation part of the CopyAttention model\n",
    "        # print(\"======= Training with batch size = %d =======\"%FLAGS.batch_size)\n",
    "        predict = model.inference(FLAGS.batch_size, context_pl, zp_pl, zm_pl,\n",
    "                                  gf_pl, gw_pl, copy_pl, proj_pl)\n",
    "        loss = model.loss(predict, next_pl) # cross_entropy\n",
    "        train_op = model.training(loss) # optimizer \n",
    "        evaluate = model.evaluate(predict, next_pl)\n",
    "        # print(\"Train Accuracy: \", evaluate)\n",
    "        # print(\"======= Stop Training =======\")\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Test component of the model\n",
    "        # print(\"======= Testing model with batch size = 1 =======\")\n",
    "        pred_single = model.inference(1, context_plt, zp_plt, zm_plt,\n",
    "                                      gf_plt, gw_plt, copy_plt,\n",
    "                                      proj_plt)\n",
    "        predicted_label = model.predict(pred_single) #label, softmax pred_single\n",
    "        ##\n",
    "\n",
    "\n",
    "\n",
    "        ## Initialize the variables and start the session\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        # ckpt_file = os.path.join('/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint','15', '16.ckpt')\n",
    "        # saver.restore(sess, r'/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint/15/16.ckpt')\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "        # train_loss_epoch = []\n",
    "        \n",
    "        for epoch in range(1, FLAGS.num_epochs + 1):\n",
    "            train_loss_tot = []\n",
    "            train_perplex_tot = []\n",
    "            train_dataset.generate_permutation()\n",
    "            start_e = time.time()\n",
    "            for i in range(num_train_examples):\n",
    "                try:\n",
    "                    feed_dict = fill_feed_dict(train_dataset, i,\n",
    "                                              context_pl, zp_pl, zm_pl,\n",
    "                                              gf_pl, gw_pl, next_pl,\n",
    "                                              copy_pl, proj_pl)\n",
    "                    _, loss_value = sess.run([train_op, loss],\n",
    "                                            feed_dict=feed_dict)\n",
    "                    \n",
    "                    \n",
    "                    # Train perplexity\n",
    "                    train_perplexity_fn = tf.exp(loss_value)\n",
    "                    train_perplexity = sess.run(train_perplexity_fn)\n",
    "\n",
    "                    train_loss_tot.append(loss_value)\n",
    "                    train_perplex_tot.append(train_perplexity)\n",
    "\n",
    "                    if i % FLAGS.print_every == 0:\n",
    "                        print(\"Epoch : %d\\tStep : %d\\tLoss : %0.3f\\tPerplexity : %3f\" % (epoch, i, loss_value, train_perplexity))\n",
    "\n",
    "                    if i == -1 and i % FLAGS.valid_every == 0:\n",
    "                        print(\"Validation starting\")\n",
    "                        #### TEST\n",
    "                        valid_loss = do_eval(sess, train_op, loss,\n",
    "                                            valid_dataset,\n",
    "                                            context_pl, zp_pl, zm_pl,\n",
    "                                            gf_pl, gw_pl, next_pl,\n",
    "                                            copy_pl, proj_pl)\n",
    "                        \n",
    "                        # Valid perplexity\n",
    "                        valid_perplexity_fn = tf.exp(valid_loss)\n",
    "                        valid_perplexity = sess.run(valid_perplexity_fn)\n",
    "\n",
    "                        print(\"Epoch : %d\\tValidation loss: %0.5f\\tValid Perplexity : %5f\" % (i, valid_loss, valid_perplexity))\n",
    "\n",
    "                    if i != 0 and i % FLAGS.sample_every == 0:\n",
    "                        # print(\"Test starting\")\n",
    "                        test_dataset.reset_context()\n",
    "                        pos = 0\n",
    "                        len_sent = 0\n",
    "                        prev_predict = word2idx['<start>']\n",
    "                        res_path = os.path.join(expt_result_path, 'sample.txt')\n",
    "                        print(\"Write every 1000 sample in txt\")\n",
    "                        with open(res_path, 'a') as exp:\n",
    "                            while pos != 1:\n",
    "                                try:\n",
    "                                  #### TEST\n",
    "                                    feed_dict_t, idx2wq = fill_feed_dict_single(test_dataset,\n",
    "                                                                                prev_predict,\n",
    "                                                                                0, context_plt,\n",
    "                                                                                zp_plt, zm_plt,\n",
    "                                                                                gf_plt, gw_plt,\n",
    "                                                                                next_plt,\n",
    "                                                                                copy_plt,\n",
    "                                                                                proj_plt)\n",
    "                                    prev_predict = sess.run([predicted_label],\n",
    "                                                            feed_dict=feed_dict_t)\n",
    "                                    prev = prev_predict[0][0][0]\n",
    "                                    if prev in idx2wq:\n",
    "                                        exp.write(idx2wq[prev] + ' ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    else:\n",
    "                                        exp.write('<unk> ')\n",
    "                                        len_sent = len_sent + 1\n",
    "                                    if prev == word2idx['.']:\n",
    "                                        pos = 1\n",
    "                                        exp.write('\\n')\n",
    "                                    if len_sent == 50:\n",
    "                                        break\n",
    "                                    prev_predict = prev\n",
    "                                except:\n",
    "                                    pass\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # train_loss_epoch.append(train_loss_epoch.mean())\n",
    "            duration_e = time.time() - start_e\n",
    "            print(\"Time taken for epoch : %d is %.3f minutes\" % (epoch, duration_e/60))\n",
    "\n",
    "\n",
    "            train_res = os.path.join(expt_result_path, 'train_loss.txt')\n",
    "            with open(train_res, 'a') as tloss_f:\n",
    "                tloss_f.write(\"Epoch : %d\\tTrain loss: %0.5f\\tTrain perplexity: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, np.mean(train_loss_tot), np.mean(train_perplex_tot), duration_e))\n",
    "\n",
    "            print(\"Saving checkpoint for epoch %d\" % (epoch))\n",
    "            checkpoint_file = os.path.join(chkpt_result_path, str(epoch) + '.ckpt')\n",
    "            saver.save(sess, checkpoint_file)\n",
    "\n",
    "            print(\"Validation starting\")\n",
    "            start = time.time()\n",
    "            valid_loss = do_eval(sess, train_op, loss,\n",
    "                                 valid_dataset, context_pl,\n",
    "                                 zp_pl, zm_pl, gf_pl, gw_pl,\n",
    "                                 next_pl, copy_pl, proj_pl)\n",
    " \n",
    "            # Valid perplexity\n",
    "            valid_perplexity_fn = tf.exp(valid_loss)\n",
    "            valid_perplexity = sess.run(valid_perplexity_fn)\n",
    "\n",
    "            duration = time.time() - start\n",
    "            print(\"Epoch : %d\\tValidation loss: %0.5f\\tValidation perplexity: %0.5f\" % (epoch, valid_loss,valid_perplexity))\n",
    "            print(\"Time taken for validating epoch %d : %0.3f\" % (epoch, duration))\n",
    "            valid_res = os.path.join(expt_result_path, 'valid_loss.txt')\n",
    "            \n",
    "            with open(valid_res, 'a') as vloss_f:\n",
    "                vloss_f.write(\"Epoch : %d\\tValidation loss: %0.5f\\tValidation perplexity: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, valid_loss, valid_perplexity, duration))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGyEQdKUMp_V",
    "outputId": "44c07ec8-adaa-436a-8cc7-781762e3168c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating w_index\n",
      "Creating field indexes\n",
      "Field vocabulary size : 1689\n",
      "Processed fields in 51.519 s\n",
      "Creating table word index\n",
      "Created the table word index in 24.915 s\n",
      "num_train_examples:  35766\n",
      "num_valid_examples:  8951\n",
      "num_test_examples:  8935\n",
      "Initializing the CopyAttention model\n",
      "WARNING:tensorflow:From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:55: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:18.651765 140454726940544 deprecation.py:323] From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:55: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:67: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:18.654552 140454726940544 module_wrapper.py:139] From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:67: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:18.657124 140454726940544 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "I1208 14:09:19.145890 140454726940544 utils.py:141] NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing the CopyAttention model\n",
      "======= CopyAttention model done in 0.051 minutes. =======\n",
      "WARNING:tensorflow:From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/feed_dicts.py:26: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:20.164796 140454726940544 module_wrapper.py:139] From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/feed_dicts.py:26: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Verify placeholders: context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl: =======\n",
      "(32, 13)\n",
      "(32, 130)\n",
      "(32, 130)\n",
      "(32, 10)\n",
      "(32, 337)\n",
      "(?, 10)\n",
      "(?, 20337)\n",
      "WARNING:tensorflow:From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:335: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:20.204188 140454726940544 module_wrapper.py:139] From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:335: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAGRAD op\n",
      "WARNING:tensorflow:From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:378: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:20.229916 140454726940544 module_wrapper.py:139] From /content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/src/copyattention.py:378: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:20.525364 140454726940544 deprecation.py:506] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 14:09:20.659737 140454726940544 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed 42\n",
      "Epoch : 1\tStep : 0\tLoss : 9.901\tPerplexity : 19950.787109\n",
      "Epoch : 1\tStep : 100\tLoss : 9.494\tPerplexity : 13277.446289\n",
      "Epoch : 1\tStep : 200\tLoss : 7.652\tPerplexity : 2103.806396\n",
      "Epoch : 1\tStep : 300\tLoss : 8.493\tPerplexity : 4880.969727\n",
      "Epoch : 1\tStep : 500\tLoss : 7.387\tPerplexity : 1614.537231\n",
      "Epoch : 1\tStep : 600\tLoss : 4.267\tPerplexity : 71.321556\n",
      "Epoch : 1\tStep : 700\tLoss : 7.295\tPerplexity : 1473.339844\n",
      "Epoch : 1\tStep : 800\tLoss : 6.488\tPerplexity : 657.512878\n",
      "Epoch : 1\tStep : 900\tLoss : 3.979\tPerplexity : 53.461407\n",
      "Epoch : 1\tStep : 1000\tLoss : 5.509\tPerplexity : 246.948563\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 1100\tLoss : 3.190\tPerplexity : 24.284740\n",
      "Epoch : 1\tStep : 1200\tLoss : 5.839\tPerplexity : 343.587311\n",
      "Epoch : 1\tStep : 1300\tLoss : 6.716\tPerplexity : 825.382202\n",
      "Epoch : 1\tStep : 1400\tLoss : 2.631\tPerplexity : 13.892287\n",
      "Epoch : 1\tStep : 1500\tLoss : 3.064\tPerplexity : 21.414185\n",
      "Epoch : 1\tStep : 1700\tLoss : 2.036\tPerplexity : 7.659359\n",
      "Epoch : 1\tStep : 1800\tLoss : 5.897\tPerplexity : 364.028809\n",
      "Epoch : 1\tStep : 1900\tLoss : 9.198\tPerplexity : 9874.434570\n",
      "Epoch : 1\tStep : 2000\tLoss : 4.979\tPerplexity : 145.330933\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 2100\tLoss : 6.037\tPerplexity : 418.775665\n",
      "Epoch : 1\tStep : 2200\tLoss : 5.335\tPerplexity : 207.420929\n",
      "Epoch : 1\tStep : 2300\tLoss : 5.431\tPerplexity : 228.326935\n",
      "Epoch : 1\tStep : 2400\tLoss : 3.660\tPerplexity : 38.856834\n",
      "Epoch : 1\tStep : 2500\tLoss : 7.315\tPerplexity : 1502.963501\n",
      "Epoch : 1\tStep : 2600\tLoss : 6.942\tPerplexity : 1034.895508\n",
      "Epoch : 1\tStep : 2700\tLoss : 5.043\tPerplexity : 155.000000\n",
      "Epoch : 1\tStep : 2800\tLoss : 4.493\tPerplexity : 89.348045\n",
      "Epoch : 1\tStep : 2900\tLoss : 4.849\tPerplexity : 127.653305\n",
      "Epoch : 1\tStep : 3000\tLoss : 6.262\tPerplexity : 524.115234\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 3100\tLoss : 2.684\tPerplexity : 14.643900\n",
      "Epoch : 1\tStep : 3200\tLoss : 5.299\tPerplexity : 200.188736\n",
      "Epoch : 1\tStep : 3400\tLoss : 5.942\tPerplexity : 380.642151\n",
      "Epoch : 1\tStep : 3700\tLoss : 3.791\tPerplexity : 44.280327\n",
      "Epoch : 1\tStep : 3800\tLoss : 2.470\tPerplexity : 11.820348\n",
      "Epoch : 1\tStep : 3900\tLoss : 2.295\tPerplexity : 9.927877\n",
      "Epoch : 1\tStep : 4000\tLoss : 4.444\tPerplexity : 85.093681\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 4100\tLoss : 2.083\tPerplexity : 8.028022\n",
      "Epoch : 1\tStep : 4200\tLoss : 4.437\tPerplexity : 84.542381\n",
      "Epoch : 1\tStep : 4300\tLoss : 4.849\tPerplexity : 127.676437\n",
      "Epoch : 1\tStep : 4500\tLoss : 3.459\tPerplexity : 31.795321\n",
      "Epoch : 1\tStep : 4600\tLoss : 3.243\tPerplexity : 25.617357\n",
      "Epoch : 1\tStep : 4700\tLoss : 0.322\tPerplexity : 1.380569\n",
      "Epoch : 1\tStep : 4800\tLoss : 3.313\tPerplexity : 27.464214\n",
      "Epoch : 1\tStep : 4900\tLoss : 6.377\tPerplexity : 588.381531\n",
      "Epoch : 1\tStep : 5000\tLoss : 6.004\tPerplexity : 405.226440\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 5100\tLoss : 4.214\tPerplexity : 67.644615\n",
      "Epoch : 1\tStep : 5200\tLoss : 2.137\tPerplexity : 8.474202\n",
      "Epoch : 1\tStep : 5400\tLoss : 8.172\tPerplexity : 3539.786133\n",
      "Epoch : 1\tStep : 5500\tLoss : 4.580\tPerplexity : 97.513367\n",
      "Epoch : 1\tStep : 5600\tLoss : 3.983\tPerplexity : 53.662628\n",
      "Epoch : 1\tStep : 5700\tLoss : 2.613\tPerplexity : 13.637280\n",
      "Epoch : 1\tStep : 5800\tLoss : 1.698\tPerplexity : 5.463890\n",
      "Epoch : 1\tStep : 5900\tLoss : 2.619\tPerplexity : 13.725351\n",
      "Epoch : 1\tStep : 6000\tLoss : 3.042\tPerplexity : 20.944151\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 6200\tLoss : 2.142\tPerplexity : 8.520415\n",
      "Epoch : 1\tStep : 6300\tLoss : 2.093\tPerplexity : 8.105585\n",
      "Epoch : 1\tStep : 6400\tLoss : 5.616\tPerplexity : 274.697296\n",
      "Epoch : 1\tStep : 6500\tLoss : 3.293\tPerplexity : 26.921946\n",
      "Epoch : 1\tStep : 6600\tLoss : 2.863\tPerplexity : 17.520809\n",
      "Epoch : 1\tStep : 6700\tLoss : 4.261\tPerplexity : 70.874214\n",
      "Epoch : 1\tStep : 6800\tLoss : 4.311\tPerplexity : 74.547867\n",
      "Epoch : 1\tStep : 6900\tLoss : 7.197\tPerplexity : 1336.028687\n",
      "Epoch : 1\tStep : 7100\tLoss : 2.721\tPerplexity : 15.199691\n",
      "Epoch : 1\tStep : 7200\tLoss : 4.518\tPerplexity : 91.641418\n",
      "Epoch : 1\tStep : 7300\tLoss : 3.428\tPerplexity : 30.817787\n",
      "Epoch : 1\tStep : 7600\tLoss : 2.216\tPerplexity : 9.172968\n",
      "Epoch : 1\tStep : 7700\tLoss : 2.742\tPerplexity : 15.513553\n",
      "Epoch : 1\tStep : 7800\tLoss : 4.441\tPerplexity : 84.887199\n",
      "Epoch : 1\tStep : 7900\tLoss : 0.358\tPerplexity : 1.429771\n",
      "Epoch : 1\tStep : 8000\tLoss : 6.157\tPerplexity : 471.789215\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 8100\tLoss : 3.940\tPerplexity : 51.431282\n",
      "Epoch : 1\tStep : 8200\tLoss : 2.507\tPerplexity : 12.271405\n",
      "Epoch : 1\tStep : 8300\tLoss : 5.046\tPerplexity : 155.475143\n",
      "Epoch : 1\tStep : 8500\tLoss : 5.211\tPerplexity : 183.356430\n",
      "Epoch : 1\tStep : 8800\tLoss : 2.776\tPerplexity : 16.052557\n",
      "Epoch : 1\tStep : 8900\tLoss : 5.717\tPerplexity : 304.071320\n",
      "Epoch : 1\tStep : 9000\tLoss : 1.239\tPerplexity : 3.452481\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 9100\tLoss : 1.799\tPerplexity : 6.041513\n",
      "Epoch : 1\tStep : 9200\tLoss : 1.621\tPerplexity : 5.055781\n",
      "Epoch : 1\tStep : 9300\tLoss : 2.483\tPerplexity : 11.979867\n",
      "Epoch : 1\tStep : 9400\tLoss : 3.401\tPerplexity : 29.995296\n",
      "Epoch : 1\tStep : 9500\tLoss : 4.294\tPerplexity : 73.291847\n",
      "Epoch : 1\tStep : 9700\tLoss : 5.024\tPerplexity : 151.998764\n",
      "Epoch : 1\tStep : 9800\tLoss : 2.237\tPerplexity : 9.366966\n",
      "Epoch : 1\tStep : 10100\tLoss : 5.070\tPerplexity : 159.242981\n",
      "Epoch : 1\tStep : 10200\tLoss : 5.847\tPerplexity : 346.365509\n",
      "Epoch : 1\tStep : 10300\tLoss : 3.571\tPerplexity : 35.538612\n",
      "Epoch : 1\tStep : 10400\tLoss : 1.574\tPerplexity : 4.827780\n",
      "Epoch : 1\tStep : 10500\tLoss : 6.917\tPerplexity : 1009.546326\n",
      "Epoch : 1\tStep : 10600\tLoss : 3.452\tPerplexity : 31.577160\n",
      "Epoch : 1\tStep : 10700\tLoss : 6.398\tPerplexity : 600.687927\n",
      "Epoch : 1\tStep : 10800\tLoss : 6.755\tPerplexity : 858.573181\n",
      "Epoch : 1\tStep : 10900\tLoss : 2.422\tPerplexity : 11.268168\n",
      "Epoch : 1\tStep : 11100\tLoss : 6.912\tPerplexity : 1003.917786\n",
      "Epoch : 1\tStep : 11400\tLoss : 5.431\tPerplexity : 228.334549\n",
      "Epoch : 1\tStep : 11500\tLoss : 1.775\tPerplexity : 5.900854\n",
      "Epoch : 1\tStep : 11600\tLoss : 2.965\tPerplexity : 19.388437\n",
      "Epoch : 1\tStep : 11700\tLoss : 4.252\tPerplexity : 70.255974\n",
      "Epoch : 1\tStep : 11800\tLoss : 3.163\tPerplexity : 23.651649\n",
      "Epoch : 1\tStep : 12000\tLoss : 1.704\tPerplexity : 5.496819\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 12100\tLoss : 3.264\tPerplexity : 26.157011\n",
      "Epoch : 1\tStep : 12200\tLoss : 2.259\tPerplexity : 9.569269\n",
      "Epoch : 1\tStep : 12300\tLoss : 3.457\tPerplexity : 31.717064\n",
      "Epoch : 1\tStep : 12400\tLoss : 3.871\tPerplexity : 47.983570\n",
      "Epoch : 1\tStep : 12500\tLoss : 4.744\tPerplexity : 114.867981\n",
      "Epoch : 1\tStep : 12600\tLoss : 2.857\tPerplexity : 17.409269\n",
      "Epoch : 1\tStep : 12700\tLoss : 2.600\tPerplexity : 13.464334\n",
      "Epoch : 1\tStep : 12800\tLoss : 1.534\tPerplexity : 4.634517\n",
      "Epoch : 1\tStep : 12900\tLoss : 4.336\tPerplexity : 76.436920\n",
      "Epoch : 1\tStep : 13000\tLoss : 3.846\tPerplexity : 46.809113\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 13100\tLoss : 0.969\tPerplexity : 2.636614\n",
      "Epoch : 1\tStep : 13200\tLoss : 4.417\tPerplexity : 82.880585\n",
      "Epoch : 1\tStep : 13300\tLoss : 4.758\tPerplexity : 116.522598\n",
      "Epoch : 1\tStep : 13400\tLoss : 3.249\tPerplexity : 25.764294\n",
      "Epoch : 1\tStep : 13500\tLoss : 0.161\tPerplexity : 1.175271\n",
      "Epoch : 1\tStep : 13600\tLoss : 4.489\tPerplexity : 89.047089\n",
      "Epoch : 1\tStep : 13700\tLoss : 2.011\tPerplexity : 7.467651\n",
      "Epoch : 1\tStep : 13800\tLoss : 6.231\tPerplexity : 508.499084\n",
      "Epoch : 1\tStep : 13900\tLoss : 5.176\tPerplexity : 176.942551\n",
      "Epoch : 1\tStep : 14000\tLoss : 3.846\tPerplexity : 46.805935\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 14100\tLoss : 0.468\tPerplexity : 1.596241\n",
      "Epoch : 1\tStep : 14300\tLoss : 4.398\tPerplexity : 81.299782\n",
      "Epoch : 1\tStep : 14500\tLoss : 4.374\tPerplexity : 79.340279\n",
      "Epoch : 1\tStep : 14600\tLoss : 2.972\tPerplexity : 19.536940\n",
      "Epoch : 1\tStep : 14800\tLoss : 5.657\tPerplexity : 286.294525\n",
      "Epoch : 1\tStep : 14900\tLoss : 5.526\tPerplexity : 251.256775\n",
      "Epoch : 1\tStep : 15000\tLoss : 5.456\tPerplexity : 234.235382\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 15100\tLoss : 6.408\tPerplexity : 606.483276\n",
      "Epoch : 1\tStep : 15200\tLoss : 2.627\tPerplexity : 13.838430\n",
      "Epoch : 1\tStep : 15400\tLoss : 2.950\tPerplexity : 19.106255\n",
      "Epoch : 1\tStep : 15500\tLoss : 2.951\tPerplexity : 19.121412\n",
      "Epoch : 1\tStep : 15600\tLoss : 2.517\tPerplexity : 12.386348\n",
      "Epoch : 1\tStep : 15700\tLoss : 3.288\tPerplexity : 26.796568\n",
      "Epoch : 1\tStep : 15800\tLoss : 3.873\tPerplexity : 48.108959\n",
      "Epoch : 1\tStep : 15900\tLoss : 2.488\tPerplexity : 12.042269\n",
      "Epoch : 1\tStep : 16100\tLoss : 3.406\tPerplexity : 30.145601\n",
      "Epoch : 1\tStep : 16200\tLoss : 2.352\tPerplexity : 10.511420\n",
      "Epoch : 1\tStep : 16300\tLoss : 1.639\tPerplexity : 5.149259\n",
      "Epoch : 1\tStep : 16400\tLoss : 4.532\tPerplexity : 92.954994\n",
      "Epoch : 1\tStep : 16500\tLoss : 3.166\tPerplexity : 23.715004\n",
      "Epoch : 1\tStep : 16600\tLoss : 1.702\tPerplexity : 5.485930\n",
      "Epoch : 1\tStep : 16800\tLoss : 2.126\tPerplexity : 8.379098\n",
      "Epoch : 1\tStep : 17000\tLoss : 3.191\tPerplexity : 24.306654\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 17200\tLoss : 5.396\tPerplexity : 220.560608\n",
      "Epoch : 1\tStep : 17300\tLoss : 5.145\tPerplexity : 171.503586\n",
      "Epoch : 1\tStep : 17400\tLoss : 3.479\tPerplexity : 32.419590\n",
      "Epoch : 1\tStep : 17500\tLoss : 2.927\tPerplexity : 18.665627\n",
      "Epoch : 1\tStep : 17600\tLoss : 3.345\tPerplexity : 28.351753\n",
      "Epoch : 1\tStep : 17700\tLoss : 2.292\tPerplexity : 9.892389\n",
      "Epoch : 1\tStep : 17800\tLoss : 8.430\tPerplexity : 4581.828613\n",
      "Epoch : 1\tStep : 17900\tLoss : 4.016\tPerplexity : 55.485985\n",
      "Epoch : 1\tStep : 18000\tLoss : 2.260\tPerplexity : 9.583007\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 18100\tLoss : 1.707\tPerplexity : 5.511453\n",
      "Epoch : 1\tStep : 18200\tLoss : 2.329\tPerplexity : 10.269382\n",
      "Epoch : 1\tStep : 18300\tLoss : 4.436\tPerplexity : 84.415253\n",
      "Epoch : 1\tStep : 18500\tLoss : 4.828\tPerplexity : 124.913673\n",
      "Epoch : 1\tStep : 18600\tLoss : 1.516\tPerplexity : 4.553697\n",
      "Epoch : 1\tStep : 18700\tLoss : 2.658\tPerplexity : 14.265572\n",
      "Epoch : 1\tStep : 18800\tLoss : 5.811\tPerplexity : 334.094635\n",
      "Epoch : 1\tStep : 18900\tLoss : 1.020\tPerplexity : 2.773553\n",
      "Epoch : 1\tStep : 19000\tLoss : 5.505\tPerplexity : 245.796555\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 19100\tLoss : 5.471\tPerplexity : 237.796555\n",
      "Epoch : 1\tStep : 19200\tLoss : 2.158\tPerplexity : 8.655616\n",
      "Epoch : 1\tStep : 19300\tLoss : 5.277\tPerplexity : 195.875854\n",
      "Epoch : 1\tStep : 19400\tLoss : 4.576\tPerplexity : 97.100685\n",
      "Epoch : 1\tStep : 19500\tLoss : 4.836\tPerplexity : 125.990128\n",
      "Epoch : 1\tStep : 19600\tLoss : 3.887\tPerplexity : 48.753555\n",
      "Epoch : 1\tStep : 19800\tLoss : 7.207\tPerplexity : 1348.488892\n",
      "Epoch : 1\tStep : 19900\tLoss : 3.373\tPerplexity : 29.172462\n",
      "Epoch : 1\tStep : 20000\tLoss : 3.307\tPerplexity : 27.297291\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 20100\tLoss : 2.179\tPerplexity : 8.834666\n",
      "Epoch : 1\tStep : 20200\tLoss : 5.579\tPerplexity : 264.837616\n",
      "Epoch : 1\tStep : 20300\tLoss : 2.867\tPerplexity : 17.585609\n",
      "Epoch : 1\tStep : 20400\tLoss : 5.175\tPerplexity : 176.859802\n",
      "Epoch : 1\tStep : 20500\tLoss : 1.076\tPerplexity : 2.933329\n",
      "Epoch : 1\tStep : 20600\tLoss : 4.199\tPerplexity : 66.636925\n",
      "Epoch : 1\tStep : 20700\tLoss : 3.697\tPerplexity : 40.327240\n",
      "Epoch : 1\tStep : 20800\tLoss : 4.738\tPerplexity : 114.170593\n",
      "Epoch : 1\tStep : 20900\tLoss : 0.753\tPerplexity : 2.123728\n",
      "Epoch : 1\tStep : 21000\tLoss : 1.273\tPerplexity : 3.570047\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 21100\tLoss : 5.678\tPerplexity : 292.489899\n",
      "Epoch : 1\tStep : 21200\tLoss : 1.885\tPerplexity : 6.589262\n",
      "Epoch : 1\tStep : 21300\tLoss : 3.060\tPerplexity : 21.319412\n",
      "Epoch : 1\tStep : 21500\tLoss : 4.130\tPerplexity : 62.201061\n",
      "Epoch : 1\tStep : 21600\tLoss : 6.243\tPerplexity : 514.533875\n",
      "Epoch : 1\tStep : 21800\tLoss : 4.893\tPerplexity : 133.332321\n",
      "Epoch : 1\tStep : 21900\tLoss : 5.745\tPerplexity : 312.504059\n",
      "Epoch : 1\tStep : 22000\tLoss : 2.361\tPerplexity : 10.606038\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 22100\tLoss : 2.571\tPerplexity : 13.082812\n",
      "Epoch : 1\tStep : 22200\tLoss : 4.584\tPerplexity : 97.930130\n",
      "Epoch : 1\tStep : 22400\tLoss : 2.901\tPerplexity : 18.193960\n",
      "Epoch : 1\tStep : 22600\tLoss : 4.680\tPerplexity : 107.735374\n",
      "Epoch : 1\tStep : 22700\tLoss : 1.655\tPerplexity : 5.231227\n",
      "Epoch : 1\tStep : 22800\tLoss : 1.679\tPerplexity : 5.360649\n",
      "Epoch : 1\tStep : 22900\tLoss : 4.705\tPerplexity : 110.456291\n",
      "Epoch : 1\tStep : 23200\tLoss : 4.887\tPerplexity : 132.595657\n",
      "Epoch : 1\tStep : 23300\tLoss : 3.682\tPerplexity : 39.724163\n",
      "Epoch : 1\tStep : 23400\tLoss : 5.395\tPerplexity : 220.251938\n",
      "Epoch : 1\tStep : 23500\tLoss : 3.156\tPerplexity : 23.469891\n",
      "Epoch : 1\tStep : 23600\tLoss : 1.835\tPerplexity : 6.267891\n",
      "Epoch : 1\tStep : 23700\tLoss : 1.373\tPerplexity : 3.945580\n",
      "Epoch : 1\tStep : 23800\tLoss : 1.545\tPerplexity : 4.687209\n",
      "Epoch : 1\tStep : 23900\tLoss : 3.117\tPerplexity : 22.579309\n",
      "Epoch : 1\tStep : 24000\tLoss : 1.919\tPerplexity : 6.815805\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 24200\tLoss : 1.833\tPerplexity : 6.250535\n",
      "Epoch : 1\tStep : 24300\tLoss : 2.696\tPerplexity : 14.825832\n",
      "Epoch : 1\tStep : 24400\tLoss : 7.160\tPerplexity : 1287.407227\n",
      "Epoch : 1\tStep : 24500\tLoss : 6.221\tPerplexity : 503.240845\n",
      "Epoch : 1\tStep : 24600\tLoss : 2.321\tPerplexity : 10.188440\n",
      "Epoch : 1\tStep : 24800\tLoss : 1.772\tPerplexity : 5.882447\n",
      "Epoch : 1\tStep : 24900\tLoss : 2.763\tPerplexity : 15.844522\n",
      "Epoch : 1\tStep : 25000\tLoss : 5.690\tPerplexity : 295.758087\n",
      "Write every 1000 sample in txt\n",
      "Epoch : 1\tStep : 25100\tLoss : 3.184\tPerplexity : 24.131969\n",
      "Epoch : 1\tStep : 25200\tLoss : 1.267\tPerplexity : 3.548587\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HKNiC0IyPap"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import os\n",
    "def split(sentence): \n",
    "    return (sentence.split())\n",
    "\n",
    "def compute_BLEU(sentence, ref):\n",
    "  reference = split(ref)\n",
    "  # if sentence == '. \\n':\n",
    "  #     return 0\n",
    "  # else:\n",
    "  if '-lrb-' in sentence:\n",
    "      reference.append('-lrb-')\n",
    "  if '-rrb-' in sentence:\n",
    "      reference.append('-rrb-')\n",
    "\n",
    "  candidate = split(sentence)\n",
    "  print(reference)\n",
    "  print(candidate)\n",
    "  BLEU_score = sentence_bleu([reference], candidate)\n",
    "  return BLEU_score\n",
    "\n",
    "epoch = 1\n",
    "BLEU_per_epoch = np.zeros(10)\n",
    "expt_result_path = '/content/drive/MyDrive/GM5/DEEP/Paper/experiment/39'\n",
    "reference = 'leonard shenoff randle -lrb- born february 12 , 1949 -rrb- is a former major league baseball player .'\n",
    "res_path = os.path.join(expt_result_path, 'sample%d.txt'%epoch)\n",
    "file =  open(res_path, \"r\")\n",
    "Bleu = 0.\n",
    "nb = 0\n",
    "for sentence in file:\n",
    "    bleu = compute_BLEU(sentence, reference)\n",
    "    print(\"bleu: %4f\" % bleu)\n",
    "    Bleu += bleu\n",
    "    nb += 1\n",
    "\n",
    "BLEU_per_epoch[epoch-1] = Bleu /  nb  \n",
    "print(\"BLEU metric for epoch %d : %4f\" % (epoch, BLEU_per_epoch[epoch-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7YISjXKp6Xd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def main(_):\n",
    "#     # pprint(flags.FLAGS.__flags)\n",
    "\n",
    "#     #### experiment_dir set : choose the last experiment file.\n",
    "#     if not os.path.exists(FLAGS.experiment_dir):\n",
    "#         os.makedirs(FLAGS.experiment_dir)\n",
    "#         expt_num = \"1\"\n",
    "#     else:\n",
    "#         expts = os.listdir(FLAGS.experiment_dir)\n",
    "#         last_expr = max([int(folder) for folder in expts])\n",
    "#         expt_num = str(last_expr + 1)\n",
    "#     expt_result_path = os.path.join(FLAGS.experiment_dir, expt_num)\n",
    "#     os.makedirs(expt_result_path)\n",
    "#     ####\n",
    "\n",
    "#     #### checkpoint_dir path set\n",
    "#     if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "#         os.makedirs(FLAGS.checkpoint_dir)\n",
    "#     chkpt_result_path = os.path.join(FLAGS.checkpoint_dir, expt_num)\n",
    "#     os.makedirs(chkpt_result_path)\n",
    "#     ####\n",
    "\n",
    "#     #### 记录 flags.FLAGS的参数 --> write them into params.json\n",
    "#     params_e_path = os.path.join(expt_result_path, \"params.json\")\n",
    "#     params_c_path = os.path.join(chkpt_result_path, \"params.json\")\n",
    "#     with open(params_e_path, 'w') as params_e, open(params_c_path, 'w') as params_c:\n",
    "#         json.dump(flags.FLAGS.__flags, params_e,cls= MyEncoder)\n",
    "#         json.dump(flags.FLAGS.__flags, params_c,cls= MyEncoder)\n",
    "#     ####\n",
    "\n",
    "#     #### Generate the indexes for create train/valid/test dataset objects\n",
    "#     word2idx, field2idx, qword2idx, nF, max_words_in_table, word_set = \\\n",
    "#         setup(FLAGS.data_dir, '/content/drive/MyDrive/GM5/DEEP/Paper/NeuralTextGeneration-master/data/embeddings', FLAGS.n, FLAGS.batch_size,FLAGS.nW, FLAGS.min_field_freq, FLAGS.nQ)\n",
    "\n",
    "#     train_dataset = DataSet(FLAGS.data_dir, 'train', FLAGS.n, FLAGS.nW, nF,\n",
    "#                             FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "#                             field2idx, qword2idx,\n",
    "#                             FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "#                             max_words_in_table, word_set)\n",
    "\n",
    "    \n",
    "#     num_train_examples = int(train_dataset.num_examples()/10)  ##### on prend seulement n% de dataset\n",
    "\n",
    "#     valid_dataset = DataSet(FLAGS.data_dir, 'valid', FLAGS.n, FLAGS.nW, nF,\n",
    "#                             FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "#                             field2idx, qword2idx,\n",
    "#                             FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "#                             max_words_in_table, word_set)\n",
    "\n",
    "#     test_dataset = DataSet(FLAGS.data_dir, 'test', FLAGS.n, FLAGS.nW, nF,\n",
    "#                             FLAGS.nQ, FLAGS.l, FLAGS.batch_size, word2idx,\n",
    "#                             field2idx, qword2idx,\n",
    "#                             FLAGS.max_fields, FLAGS.word_max_fields,\n",
    "#                             max_words_in_table, word_set)\n",
    "#     # print(test_dataset._xs[:10])\n",
    "#     # print(test_dataset._ys[:10])\n",
    "    \n",
    "#     # print('num_train_examples: ', num_train_examples)\n",
    "#     # print('num_valid_examples: ', int(.1*valid_dataset.num_examples()))\n",
    "#     # print('num_test_examples: ',int(.1*test_dataset.num_examples()))\n",
    "#     ####\n",
    "\n",
    "\n",
    "#     #### The sizes of respective conditioning variables for placeholder generation\n",
    "#     # 上下文长度 = len(n_gram) -1\n",
    "#     context_size = (FLAGS.n - 1)\n",
    "#     # local conditiong z+(-) = 上下文长度*一个word最多出现在10个fields\n",
    "#     zp_size = context_size * FLAGS.word_max_fields\n",
    "#     zm_size = context_size * FLAGS.word_max_fields\n",
    "#     # global conditiong gf = 一个infobox中最多出现的fields个数\n",
    "#     gf_size = FLAGS.max_fields\n",
    "#     # global conditiong gw_size = infobox中出现的word总个数 非FLAG\n",
    "#     gw_size = max_words_in_table\n",
    "#     # Index into the copy action embedding matrix\n",
    "#     copy_size = FLAGS.word_max_fields\n",
    "#     # Projection matrix to project out the copy action score to output vocabulary.句子中vocab的个数 + infobox中出现的word总个数 非FLAG\n",
    "#     proj_size = FLAGS.nW + max_words_in_table\n",
    "#     ####\n",
    "\n",
    "#     ######## ADDED ########\n",
    "#     reference = 'leonard shenoff randle -lrb- born february 12 , 1949 -rrb- is a former major league baseball player .'\n",
    "#     ################\n",
    "    \n",
    "#     #### Generate the TensorFlow graph\n",
    "#     with tf.Graph().as_default():\n",
    "\n",
    "#         ## Create the CopyAttention model\n",
    "#         start_c = time.time()\n",
    "#         model = CopyAttention(FLAGS.n, FLAGS.d, FLAGS.g, FLAGS.nhu,\n",
    "#                               FLAGS.nW, nF, FLAGS.nQ, FLAGS.l,\n",
    "#                               FLAGS.learning_rate, max_words_in_table,\n",
    "#                               FLAGS.max_fields, FLAGS.word_max_fields)\n",
    "#         duration_c = time.time() - start_c\n",
    "#         print(\"======= CopyAttention model done in %.3f minutes. =======\"%(duration_c/60))\n",
    "#         ##\n",
    "\n",
    "\n",
    "#         ## Placeholders for train and validation with known shape per batch\n",
    "#         # context_pl (32, context_size) ; zp_pl (32, zp_size); zm_pl (32, zm_size)\n",
    "#         # gf_pl (32, gf_size) ; gw_pl (32, gw_size); copy_pl (none, copy_size)\n",
    "#         # proj_pl (32, proj_size); next_pl True next word tensor\n",
    "#         context_pl, zp_pl, zm_pl, gf_pl, gw_pl, next_pl, copy_pl, proj_pl = \\\n",
    "#             placeholder_inputs(FLAGS.batch_size, context_size, zp_size,\n",
    "#                                 zm_size, gf_size, gw_size, copy_size,\n",
    "#                                 proj_size)\n",
    "#         # Placeholders for test\n",
    "#         context_plt, zp_plt, zm_plt, gf_plt, gw_plt, copy_plt, proj_plt, next_plt = \\\n",
    "#             placeholder_inputs_single(context_size, zp_size, zm_size,\n",
    "#                                       gf_size, gw_size, copy_size,\n",
    "#                                       proj_size)\n",
    "            \n",
    "#         print(\"======= Verify placeholders: context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl: =======\")\n",
    "#         for i in [context_pl, zp_pl, zm_pl, gf_pl, gw_pl, copy_pl, proj_pl]:\n",
    "#             print(i.get_shape())\n",
    "#         ##\n",
    "\n",
    "        \n",
    "#         ## Train and validation part of the CopyAttention model\n",
    "#         # print(\"======= Training with batch size = %d =======\"%FLAGS.batch_size)\n",
    "#         predict = model.inference(FLAGS.batch_size, context_pl, zp_pl, zm_pl,\n",
    "#                                   gf_pl, gw_pl, copy_pl, proj_pl)\n",
    "#         loss = model.loss(predict, next_pl) # cross_entropy\n",
    "#         train_op = model.training(loss) # optimizer 梯度下降\n",
    "#         evaluate = model.evaluate(predict, next_pl)\n",
    "#         # print(\"Train Accuracy: \", evaluate)\n",
    "#         # print(\"======= Stop Training =======\")\n",
    "#         ##\n",
    "\n",
    "\n",
    "\n",
    "#         ## Test component of the model\n",
    "#         # print(\"======= Testing model with batch size = 1 =======\")\n",
    "#         pred_single = model.inference(1, context_plt, zp_plt, zm_plt,\n",
    "#                                       gf_plt, gw_plt, copy_plt,\n",
    "#                                       proj_plt)\n",
    "#         predicted_label = model.predict(pred_single) #预测label, softmax pred_single\n",
    "#         ##\n",
    "\n",
    "\n",
    "\n",
    "#         ## Initialize the variables and start the session\n",
    "#         init = tf.initialize_all_variables()\n",
    "#         saver = tf.train.Saver()\n",
    "#         sess = tf.Session()\n",
    "#         # ckpt_file = os.path.join('/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint','15', '16.ckpt')\n",
    "#         # saver.restore(sess, r'/content/drive/MyDrive/GM5/DEEP/Paper/checkpoint/15/16.ckpt')\n",
    "#         sess.run(init)\n",
    "        \n",
    "#         ######## ADDED ########\n",
    "#         BLEU_per_epoch = np.zeros(FLAGS.num_epochs)\n",
    "#         ################\n",
    "        \n",
    "#         # train_loss_epoch = []\n",
    "        \n",
    "#         for epoch in range(1, FLAGS.num_epochs + 1):\n",
    "#             train_loss_tot = []\n",
    "\n",
    "#             train_dataset.generate_permutation()\n",
    "#             start_e = time.time()\n",
    "#             for i in range(num_train_examples):\n",
    "#                 try:\n",
    "#                     feed_dict = fill_feed_dict(train_dataset, i,\n",
    "#                                               context_pl, zp_pl, zm_pl,\n",
    "#                                               gf_pl, gw_pl, next_pl,\n",
    "#                                               copy_pl, proj_pl)\n",
    "#                     _, loss_value = sess.run([train_op, loss],\n",
    "#                                             feed_dict=feed_dict)\n",
    "\n",
    "#                     train_loss_tot.append(loss_value)\n",
    "\n",
    "#                     if i % FLAGS.print_every == 0:\n",
    "#                         print(\"Epoch : %d\\tStep : %d\\tLoss : %0.3f\" % (epoch, i, loss_value))\n",
    "\n",
    "#                     if i == -1 and i % FLAGS.valid_every == 0:\n",
    "#                         print(\"Validation starting\")\n",
    "#                         #### TEST\n",
    "#                         valid_loss = do_eval(sess, train_op, loss,\n",
    "#                                             valid_dataset,\n",
    "#                                             context_pl, zp_pl, zm_pl,\n",
    "#                                             gf_pl, gw_pl, next_pl,\n",
    "#                                             copy_pl, proj_pl)\n",
    "#                         print(\"Epoch : %d\\tValidation loss: %0.5f\" % (i, valid_loss))\n",
    "\n",
    "#                     if i != 0 and i % FLAGS.sample_every == 0:\n",
    "#                         # print(\"Test starting\")\n",
    "#                         test_dataset.reset_context()\n",
    "#                         pos = 0\n",
    "#                         len_sent = 0\n",
    "#                         prev_predict = word2idx['<start>']\n",
    "#                         res_path = os.path.join(expt_result_path, 'sample%d.txt'%epoch)\n",
    "#                         # print(\"Write every 1000 sample in txt\")\n",
    "#                         with open(res_path, 'a') as exp:\n",
    "#                             while pos != 1:\n",
    "#                                 try:\n",
    "#                                   #### TEST\n",
    "#                                     feed_dict_t, idx2wq = fill_feed_dict_single(test_dataset,\n",
    "#                                                                                 prev_predict,\n",
    "#                                                                                 0, context_plt,\n",
    "#                                                                                 zp_plt, zm_plt,\n",
    "#                                                                                 gf_plt, gw_plt,\n",
    "#                                                                                 next_plt,\n",
    "#                                                                                 copy_plt,\n",
    "#                                                                                 proj_plt)\n",
    "#                                     prev_predict = sess.run([predicted_label],\n",
    "#                                                             feed_dict=feed_dict_t)\n",
    "#                                     prev = prev_predict[0][0][0]\n",
    "#                                     if prev in idx2wq:\n",
    "#                                         exp.write(idx2wq[prev] + ' ')\n",
    "#                                         len_sent = len_sent + 1\n",
    "#                                     else:\n",
    "#                                         exp.write('<unk> ')\n",
    "#                                         len_sent = len_sent + 1\n",
    "#                                     if prev == word2idx['.']:\n",
    "#                                         pos = 1\n",
    "#                                         exp.write('\\n')\n",
    "#                                     if len_sent == 50:\n",
    "#                                         break\n",
    "#                                     prev_predict = prev\n",
    "#                                 except:\n",
    "#                                     pass\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "#             # res_path = os.path.join(expt_result_path, 'sample%d.txt'%epoch)\n",
    "#             # file =  open(res_path, \"r\")\n",
    "#             # Bleu = 0.\n",
    "#             # for sentence in file:\n",
    "#             #     bleu = compute_BLEU(sentence, reference)\n",
    "#             #     # print(\"bleu: %4f\" % (bleu))\n",
    "#             #     Bleu += bleu\n",
    "\n",
    "#             # train_loss_epoch.append(train_loss_epoch.mean())\n",
    "#             duration_e = time.time() - start_e\n",
    "#             print(\"Time taken for epoch : %d is %.3f minutes\" % (epoch, duration_e/60))\n",
    "\n",
    "#             ######## ADDED ########\n",
    "#             # BLEU_per_epoch[epoch-1] = Bleu /  num_train_examples  \n",
    "#             # print(\"BLEU metric for epoch %d : %4f\" % (epoch, BLEU_per_epoch[epoch-1]))\n",
    "#             ################\n",
    "\n",
    "#             # bleu_res = os.path.join(expt_result_path, 'bleu.txt')\n",
    "#             # with open(bleu_res, 'a') as bleu_f:\n",
    "#             #     bleu_f.write(\"Epoch : %d\\tBlue: %0.5f\\n\" % (epoch, BLEU_per_epoch[epoch-1]))\n",
    "\n",
    "#             train_res = os.path.join(expt_result_path, 'train_loss.txt')\n",
    "#             with open(train_res, 'a') as tloss_f:\n",
    "#                 tloss_f.write(\"Epoch : %d\\tTrain loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, np.mean(train_loss_tot), duration_e))\n",
    "\n",
    "#             print(\"Saving checkpoint for epoch %d\" % (epoch))\n",
    "#             checkpoint_file = os.path.join(chkpt_result_path, str(epoch) + '.ckpt')\n",
    "#             saver.save(sess, checkpoint_file)\n",
    "\n",
    "#             print(\"Validation starting\")\n",
    "#             start = time.time()\n",
    "#             valid_loss = do_eval(sess, train_op, loss,\n",
    "#                                  valid_dataset, context_pl,\n",
    "#                                  zp_pl, zm_pl, gf_pl, gw_pl,\n",
    "#                                  next_pl, copy_pl, proj_pl)\n",
    "#             duration = time.time() - start\n",
    "#             print(\"Epoch : %d\\tValidation loss: %0.5f\" % (epoch, valid_loss))\n",
    "#             print(\"Time taken for validating epoch %d : %0.3f\" % (epoch, duration))\n",
    "#             valid_res = os.path.join(expt_result_path, 'valid_loss.txt')\n",
    "            \n",
    "#             with open(valid_res, 'a') as vloss_f:\n",
    "#                 vloss_f.write(\"Epoch : %d\\tValidation loss: %0.5f\\tComputation time: %0.3f\\n\" % (epoch, valid_loss, duration))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqNTyoFREzCK"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# BLEU score nécessite des listes de mots\n",
    "# cette méthode prend une phrase te la transforme en une liste de mots\n",
    "def split(sentence): \n",
    "    return (sentence.split())\n",
    "\n",
    "# Générer la liste de mots de la phrase de réference et de phrase générée\n",
    "def inputs(path, sentence):\n",
    "  file =  open(path, \"r\")\n",
    "  reference = split(sentence)\n",
    "  last_line = ' '\n",
    "  for sentence in file:\n",
    "    if '-lrb-' in sentence[:8]:\n",
    "      last_line = sentence\n",
    "  \n",
    "  candidate = split(last_line)\n",
    "  reference.append('-lrb-')\n",
    "  if '-rrb-' in last_line:\n",
    "      reference.append('-rrb-')\n",
    "  return candidate, reference\n",
    "\n",
    "# path to file (variable : res_path) : On change LE PATH par la variable res_path\n",
    "path_for_test_good_sentence = '/content/drive/MyDrive/GM5/DEEP/Paper/experiment/4/sample.txt'\n",
    "# path_for_test_bad_sentence = '/content/drive/MyDrive/Université_de_Rouen_2020/Deep_Learning_Projet/paper_code/NeuralTextGeneration-master/experiment/0/sample.txt'\n",
    "\n",
    "# test examples\n",
    "#reference = split('lenny randle (born February 12, 1949) is a former Major League Baseball player.')\n",
    "#candidate = split('born february 12 , 1949 is a former professional football player. is an american beach in long beach california .') \n",
    "\n",
    "sentence = 'lenny randle (born February 12, 1949) is a former Major League Baseball player.'\n",
    "candidate, reference = inputs(path_for_test_good_sentence, sentence)\n",
    "print(reference)\n",
    "print(candidate)\n",
    "\n",
    "BLEU_score = sentence_bleu(reference, candidate)\n",
    "print(BLEU_score)\n",
    "\n",
    "\n",
    "# OUTPUT\n",
    "#['lenny', 'randle', '(born', 'February', '12,', '1949)', 'is', 'a', 'former', 'Major', 'League', 'Baseball', 'player.', '-lrb-', '-rrb-']\n",
    "#[',', '-lrb-', 'born', 'february', '12', ',', '1949', '-rrb-', 'is', 'a', 'former', 'professional', 'football', 'player', '.']\n",
    "#0.668740304976422"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Paper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
